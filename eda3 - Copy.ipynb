{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import utils, models\n",
    "\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from utils.clean_funcs.clean import remove_stopwords, make_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kapangyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapangyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapangyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to clean text\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude =set(string.punctuation)\n",
    "useless_words = ['would','could','should','le','non','federal','way','hour','lack','make','lot','getting','use','believe','thing']\n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data.xlsx')\n",
    "\n",
    "df = data[['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"']]\n",
    "df.columns = ['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>COMPONENT</th>\n",
       "      <th>SUB_COMPONENT</th>\n",
       "      <th>GRADELEVEL</th>\n",
       "      <th>SUP_STATUS</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GS-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GS-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GS-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GS-1</td>\n",
       "      <td>0</td>\n",
       "      <td>surveys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GS-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      AGENCY  \\\n",
       "0  Department of Agriculture   \n",
       "1  Department of Agriculture   \n",
       "2  Department of Agriculture   \n",
       "3  Department of Agriculture   \n",
       "4  Department of Agriculture   \n",
       "\n",
       "                                           COMPONENT SUB_COMPONENT GRADELEVEL  \\\n",
       "0  Cooperative State Research, Education, and Ext...           NaN       GS-1   \n",
       "1  Cooperative State Research, Education, and Ext...           NaN       GS-1   \n",
       "2  Cooperative State Research, Education, and Ext...           NaN       GS-1   \n",
       "3  Cooperative State Research, Education, and Ext...           NaN       GS-1   \n",
       "4  Cooperative State Research, Education, and Ext...           NaN       GS-1   \n",
       "\n",
       "   SUP_STATUS     TEXT  \n",
       "0           0      NaN  \n",
       "1           0      NaN  \n",
       "2           0      NaN  \n",
       "3           0  surveys  \n",
       "4           0      NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['TEXT'].isnull()==False]\n",
    "full_df = df[df['TEXT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['COMPONENT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['GRADELEVEL'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(subset=['TEXT'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ag = full_df[full_df['AGENCY']=='Department of Agriculture']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ag = df_ag['TEXT'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df.fillna('other')\n",
    "#full_df.replace(pd.isna,'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps = full_df['COMPONENT'].unique()\n",
    "unique_agenics = full_df['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Remove Emails\\ndata_ag = [re.sub(\\'\\\\S*@\\\\S*\\\\s?\\', \\'\\', str(sent)) for sent in data_ag]\\n\\n# Remove new line characters\\ndata_ag = [re.sub(\\'\\\\s+\\', \\' \\', str(sent)) for sent in data_ag]\\n\\n# Remove distracting single quotes\\ndata_ag = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_ag]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Remove Emails\n",
    "data_ag = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove new line characters\n",
    "data_ag = [re.sub('\\s+', ' ', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data_ag = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_ag]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "#data_ag_words = list(sent_to_words(data_ag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_ag_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build bigram and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Build the bigram and trigram models\\nbigram = gensim.models.Phrases(data_ag_words, min_count=5, threshold=100) # higher threshold fewer phrases.\\ntrigram = gensim.models.Phrases(bigram[data_ag_words], threshold=100)  \\n\\n# Faster way to get a sentence clubbed as a trigram/bigram\\nbigram_mod = gensim.models.phrases.Phraser(bigram)\\ntrigram_mod = gensim.models.phrases.Phraser(trigram)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_ag_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_ag_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(trigram_mod[bigram_mod[data_ag_words[12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, make Bigrams and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use','none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Remove Stop Words\n",
    "# data_words_nostops = remove_stopwords(data_ag_words)\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_lemmatized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to  calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Department of Agriculture':{\n",
    "'scores':{'total':8,'manager':4,'nonmanager':8\n",
    "}},\n",
    "'Department of Commerce':{\n",
    "'scores':{'total':6,'manager':4,'nonmanager':6\n",
    "}},\n",
    "'Department of Defense':{\n",
    "'scores':{'total':10,'manager':16,'nonmanager':8\n",
    "}},\n",
    "'Department of Education':{\n",
    "'scores':{'total':6,'manager':14,'nonmanager':8\n",
    "}},\n",
    "'Department of Energy':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'Department of Health and Human Services':{\n",
    "'scores':{'total':6,'manager':10,'nonmanager':10\n",
    "}},\n",
    "'Department of Homeland Security':{\n",
    "'scores':{'total':6,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of Housing and Urban Development':{\n",
    "'scores':{'total':4,'manager':10,'nonmanager':4\n",
    "}},\n",
    "'Department of Justice':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':4\n",
    "}},\n",
    "'Department of Labor':{\n",
    "'scores':{'total':8,'manager':14,'nonmanager':6\n",
    "}},\n",
    "'Department of State':{\n",
    "'scores':{'total':14,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of the Interior':{\n",
    "'scores':{'total':10,'manager':14,'nonmanager':4\n",
    "}},\n",
    "'Department of the Treasury':{\n",
    "'scores':{'total':8,'manager':10,'nonmanager':8\n",
    "}},\n",
    "'Department of Transportation':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Department of Veterans Affairs':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Environmental Protection Agency':{\n",
    "'scores':{'total':14,'manager':12,'nonmanager':10\n",
    "}},\n",
    "'General Services Administration':{\n",
    "'scores':{'total':18,'manager':8,'nonmanager':6\n",
    "}},\n",
    "'National Aeronautics and Space Administration':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'National Science Foundation':{\n",
    "'scores':{'total':4,'manager':16,'nonmanager':12\n",
    "}},\n",
    "'Nuclear Regulatory Commission':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Office of Personnel Management':{\n",
    "'scores':{'total':6,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Social Security Administration':{\n",
    "'scores':{'total':10,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Small Business Administration':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'U.S. Agency for International Development':{\n",
    "'scores':{'total':4,'manager':12,'nonmanager':8\n",
    "}\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topics for agency level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_agencies(df,topic_num):\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "\n",
    "    #model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20, step=2)\n",
    "\n",
    "    #print(coherence_values)\n",
    "    #max_coherence_score = max(coherence_values)\n",
    "    #best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    #best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df):\n",
    "\n",
    "\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "    if len(data_2)==0:\n",
    "        return 'empty'\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "    try:\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20,\\\n",
    "                   step=2,id2word=id2word2)\n",
    "    except ValueError:\n",
    "        return 'no data'\n",
    "\n",
    "    #print(coherence_values)\n",
    "    max_coherence_score = max(coherence_values)\n",
    "    best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=best_topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-150abeabfa39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_agenics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtopic_dict_agency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magency\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_topics_agencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AGENCY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0magency\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scores'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-5160d28a817f>\u001b[0m in \u001b[0;36mget_topics_agencies\u001b[1;34m(df, topic_num)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# python3 -m spacy download en\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "topic_dict_agency={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency[agency] = get_topics_agencies(full_df[full_df['AGENCY']==agency],scores.get(agency).get('scores').get('total'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency.items() ])).T\n",
    "agency_df.to_csv('Agency_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_mang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_mang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)],scores.get(agency).get('scores').get('manager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_mang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_mang.items() ])).T\n",
    "agency_df_mang.to_csv('Agency_Topics_Senior_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_nonmang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_nonmang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)],scores.get(agency).get('scores').get('nonmanager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_nonmang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_nonmang.items() ])).T\n",
    "agency_df_nonmang.to_csv('Agency_Non_Manager_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_grades = full_df['GRADELEVEL'].unique()\n",
    "grade_topic_dict = {}\n",
    "for grade in unique_grades:\n",
    "    grade_topic_dict[grade] = get_topics(full_df[full_df['GRADELEVEL']==grade])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in grade_topic_dict.items() ])).T\n",
    "gs_df.to_csv('GS_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_comps_topics_mang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_mang[agency] = temp_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_mang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_mang_df = comps_mang_df.reindex(columns = headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(comps_mang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_mang_df['topic '+str(t)].iloc[i] = comps_mang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df.to_csv('Component_SR_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics_nonmang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_nonmang[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_nonmang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_nonmang_df = comps_nonmang_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_nonmang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_nonmang_df['topic '+str(t)].iloc[i] = comps_nonmang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df.to_csv('Component_Non_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_df = comps_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_df['topic '+str(t)].iloc[i] = comps_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df.to_csv('Component_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('GS_Topics.csv','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_text = f.read()\n",
    "list1=re.sub(r\"[^a-zA-Z]+\", ' ',f_text)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('Agency_Topics.csv','r')\n",
    "g_text = g.read()\n",
    "listg=re.sub(r\"[^a-zA-Z]+\", ' ',g_text)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open('Component_SR_manager.csv','r')\n",
    "h_text = h.read()\n",
    "listh=re.sub(r\"[^a-zA-Z]+\", ' ',h_text)\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('Component_Topics.csv','r')\n",
    "j_text = j.read()\n",
    "listj=re.sub(r\"[^a-zA-Z]+\", ' ',j_text)\n",
    "j.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = open('Component_Topics.csv','r')\n",
    "k_text = k.read()\n",
    "listk=re.sub(r\"[^a-zA-Z]+\", ' ',k_text)\n",
    "k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = open('Agency_Topics_Senior_Manager.csv','r')\n",
    "l_text = l.read()\n",
    "listl=re.sub(r\"[^a-zA-Z]+\", ' ',l_text)\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word = listl+listk+listj+listh+listg+list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('words.txt',\"w\")\n",
    "text_file.write(new_word)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
