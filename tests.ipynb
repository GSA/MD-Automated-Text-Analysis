{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import utils, models\n",
    "\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from utils.clean_funcs.clean import remove_stopwords, make_bigrams, lemmatization, sent_to_words\n",
    "from utils.calc_funcs.calc import compute_coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to clean text\n",
    "# stop = set(stopwords.words('english'))\n",
    "# exclude =set(string.punctuation)\n",
    "# useless_words = ['would','could','should','le','non','federal','way','hour','lack','make','lot','getting','use','believe','thing']\n",
    "# for word in useless_words:\n",
    "#     stop.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data.xlsx')\n",
    "\n",
    "df = data[['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"']]\n",
    "df.columns = ['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['TEXT'].isnull()==False]\n",
    "full_df = df[df['TEXT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['COMPONENT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['GRADELEVEL'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(subset=['TEXT'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ag = full_df[full_df['AGENCY']=='Department of Agriculture']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ag = df_ag['TEXT'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df.fillna('other')\n",
    "#full_df.replace(pd.isna,'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps = full_df['COMPONENT'].unique()\n",
    "unique_agenics = full_df['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Remove Emails\n",
    "data_ag = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove new line characters\n",
    "data_ag = [re.sub('\\s+', ' ', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data_ag = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_ag]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_to_words(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "#data_ag_words = list(sent_to_words(data_ag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_ag_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build bigram and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_ag_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_ag_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(trigram_mod[bigram_mod[data_ag_words[12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, make Bigrams and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use','none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Remove Stop Words\n",
    "# data_words_nostops = remove_stopwords(data_ag_words)\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(data_lemmatized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to  calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Department of Agriculture':{\n",
    "'scores':{'total':8,'manager':4,'nonmanager':8\n",
    "}},\n",
    "'Department of Commerce':{\n",
    "'scores':{'total':6,'manager':4,'nonmanager':6\n",
    "}},\n",
    "'Department of Defense':{\n",
    "'scores':{'total':10,'manager':16,'nonmanager':8\n",
    "}},\n",
    "'Department of Education':{\n",
    "'scores':{'total':6,'manager':14,'nonmanager':8\n",
    "}},\n",
    "'Department of Energy':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'Department of Health and Human Services':{\n",
    "'scores':{'total':6,'manager':10,'nonmanager':10\n",
    "}},\n",
    "'Department of Homeland Security':{\n",
    "'scores':{'total':6,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of Housing and Urban Development':{\n",
    "'scores':{'total':4,'manager':10,'nonmanager':4\n",
    "}},\n",
    "'Department of Justice':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':4\n",
    "}},\n",
    "'Department of Labor':{\n",
    "'scores':{'total':8,'manager':14,'nonmanager':6\n",
    "}},\n",
    "'Department of State':{\n",
    "'scores':{'total':14,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of the Interior':{\n",
    "'scores':{'total':10,'manager':14,'nonmanager':4\n",
    "}},\n",
    "'Department of the Treasury':{\n",
    "'scores':{'total':8,'manager':10,'nonmanager':8\n",
    "}},\n",
    "'Department of Transportation':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Department of Veterans Affairs':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Environmental Protection Agency':{\n",
    "'scores':{'total':14,'manager':12,'nonmanager':10\n",
    "}},\n",
    "'General Services Administration':{\n",
    "'scores':{'total':18,'manager':8,'nonmanager':6\n",
    "}},\n",
    "'National Aeronautics and Space Administration':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'National Science Foundation':{\n",
    "'scores':{'total':4,'manager':16,'nonmanager':12\n",
    "}},\n",
    "'Nuclear Regulatory Commission':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Office of Personnel Management':{\n",
    "'scores':{'total':6,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Social Security Administration':{\n",
    "'scores':{'total':10,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Small Business Administration':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'U.S. Agency for International Development':{\n",
    "'scores':{'total':4,'manager':12,'nonmanager':8\n",
    "}\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topics for agency level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_agencies(df,topic_num):\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "\n",
    "    #model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20, step=2)\n",
    "\n",
    "    #print(coherence_values)\n",
    "    #max_coherence_score = max(coherence_values)\n",
    "    #best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    #best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df):\n",
    "\n",
    "\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "    if len(data_2)==0:\n",
    "        return 'empty'\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "    try:\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20,\\\n",
    "                   step=2,id2word=id2word2)\n",
    "    except ValueError:\n",
    "        return 'no data'\n",
    "\n",
    "    #print(coherence_values)\n",
    "    max_coherence_score = max(coherence_values)\n",
    "    best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=best_topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_dict_agency={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency[agency] = get_topics_agencies(full_df[full_df['AGENCY']==agency],scores.get(agency).get('scores').get('total'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency.items() ])).T\n",
    "agency_df.to_csv('Agency_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_mang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_mang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)],scores.get(agency).get('scores').get('manager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_mang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_mang.items() ])).T\n",
    "agency_df_mang.to_csv('Agency_Topics_Senior_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_nonmang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_nonmang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)],scores.get(agency).get('scores').get('nonmanager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_nonmang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_nonmang.items() ])).T\n",
    "agency_df_nonmang.to_csv('Agency_Non_Manager_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_grades = full_df['GRADELEVEL'].unique()\n",
    "grade_topic_dict = {}\n",
    "for grade in unique_grades:\n",
    "    grade_topic_dict[grade] = get_topics(full_df[full_df['GRADELEVEL']==grade])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in grade_topic_dict.items() ])).T\n",
    "gs_df.to_csv('GS_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_comps_topics_mang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_mang[agency] = temp_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(unique_comps_topics_mang.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics_mang['Department of Agriculture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics_mang['Department of Agriculture']['Trade and Foreign Agricultural Affairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_mang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df['topics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_mang_df = comps_mang_df.reindex(columns = headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df['topics'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(comps_mang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_mang_df['topic '+str(t)].iloc[i] = comps_mang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df.to_csv('Component_SR_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics_nonmang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_nonmang[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_nonmang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_nonmang_df = comps_nonmang_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_nonmang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_nonmang_df['topic '+str(t)].iloc[i] = comps_nonmang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df.to_csv('Component_Non_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_df = comps_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_df['topic '+str(t)].iloc[i] = comps_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df.to_csv('Component_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('GS_Topics.csv','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_text = f.read()\n",
    "list1=re.sub(r\"[^a-zA-Z]+\", ' ',f_text)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('Agency_Topics.csv','r')\n",
    "g_text = g.read()\n",
    "listg=re.sub(r\"[^a-zA-Z]+\", ' ',g_text)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open('Component_SR_manager.csv','r')\n",
    "h_text = h.read()\n",
    "listh=re.sub(r\"[^a-zA-Z]+\", ' ',h_text)\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('Component_Topics.csv','r')\n",
    "j_text = j.read()\n",
    "listj=re.sub(r\"[^a-zA-Z]+\", ' ',j_text)\n",
    "j.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = open('Component_Topics.csv','r')\n",
    "k_text = k.read()\n",
    "listk=re.sub(r\"[^a-zA-Z]+\", ' ',k_text)\n",
    "k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = open('Agency_Topics_Senior_Manager.csv','r')\n",
    "l_text = l.read()\n",
    "listl=re.sub(r\"[^a-zA-Z]+\", ' ',l_text)\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word = listl+listk+listj+listh+listg+list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('words.txt',\"w\")\n",
    "text_file.write(new_word)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "dataset = [wd for wd in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngrams_maker import NGramsMaker\n",
    "\n",
    "maker = NGramsMaker(dataset, 2)\n",
    "print(maker.make_ngrams([dataset[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker = NGramsMaker(dataset, 2)\n",
    "print(maker.make_ngrams([dataset[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topicsanalyser import TopicsAnalyser\n",
    "\n",
    "analyser = TopicsAnalyser('data.xlsx')\n",
    "# analyser = TopicsAnalyser('CSS_Hiring_Data_FedEmployee_Reason_OTHER_v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "groupby_cols = ['AGENCY','GRADELEVEL']\n",
    "\n",
    "analyser.get_topics(num_topics, groupby_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "            'key0': {\n",
    "                'key0_0': {\n",
    "                    'key0_0_0': [(0, '000'),(1, '001')]\n",
    "                },\n",
    "                'key0_1': {\n",
    "                    'key0_1_0': [(0, '010'),(1, '011')]\n",
    "                },\n",
    "                'key0_2': {\n",
    "                    'key0_2_0': [(0, '020'),(1, '021')]\n",
    "                }\n",
    "            },\n",
    "            'key1': {\n",
    "                'key1_0': {\n",
    "                    'key1_0_0': [(0, '100'),(1, '101')]\n",
    "                },\n",
    "                'key1_1': {\n",
    "                    'key1_1_0': [(0, '110'),(1, '111')]\n",
    "                },\n",
    "                'key1_2': {\n",
    "                    'key1_2_0': [(0, '120'),(1, '121')]\n",
    "                }\n",
    "            }\n",
    "       }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
